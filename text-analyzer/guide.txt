import os
from dotenv import load_dotenv

from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
from langchain.schema import BaseMessage, HumanMessage, AIMessage, SystemMessage, FunctionMessage
from langserve import add_routes

from fastapi import FastAPI


load_dotenv()

openai_api_key = os.getenv("OPENAI_API_KEY")

print(openai_api_key)
os.environ["OPENAI_API_KEY"] = openai_api_key



'''building with langchain
#langchain expression language (LCEL) is a langchain way of composing of modules used to build language applications which defines arunnable interface making it possible to seamlessly chain components

#simplest chain containe
#1. LLM/Chatmodel - reasoning engine
#2. Prompt template - instructions for language model, controls what the language model outputs
#3. output parser - converts the output of the language model into a format that can be used by the application


two types of language models, 
#1. llm - underlying model that takes a string of text and returns a string of text 
2. chatmodel - underlying model , takes a list of messages and returns a message

BaseMessage has , content of message , usually string and role , entity from which basemessage is comming 

objects to distinguish between diff roles
1. HumanMessage - comes from a human/user
2. AIMessage - comes form an AI/assistant
3. SystemMessage - comes from system
4. Function/ToolMessage = contains output of a function or tool call

5. ChatMeesage - can specify role manually

Simplest way to call llm/chatmodel is using .invoke() 
LLM.invoke: Takes in a string, returns a string.
ChatModel.invoke: Takes in a list of BaseMessage, returns a BaseMessage.'''

llm = OpenAI()
chat_model = ChatOpenAI()

text = "what would be a good company name for a company that makes colorful socks?"
messages = [HumanMessage(content=text)]

print(llm.invoke(text))
print(chat_model.invoke(messages))

''' prompt templates.
adding user input to a prompt template to provide additional context on the specific task at hand.
They bundle up all the logic from user input to fully formated prompt
'''

from langchain.prompts import PromptTemplate

prompt = PromptTemplate.from_template("What is a good name for a company that makes {product}?")
prompt.format(product="colorful socks")

from langchain.prompts.chat import ChatPromptTemplate

template = "You are a helpful assistant that translates {input_language} to {output_language}"
human_template = "{text}"

chat_prompt = ChatPromptTemplate.from_messages([("system", template), ("user", human_template)])

chat_prompt.format_messages(input_language="English", output_language="Spanish", text="I love coding")

'''output parser convert the raw output of the language model into a format that can be used by the application.
types: convert text from llm to structured info
convert chatmessage into a string
convert extra info from a call besides the message into a string
'''

from langchain.schema import BaseOutputParser

class CommaSeparatedListOutputParser(BaseOutputParser):
    """Parse the output of an LLM call to a comma-separated list."""


    def parse(self, text: str):
        """Parse the output of an LLM call."""
        return text.strip().split(", ")

# CommaSeparatedListOutputParser().parse("hi, bye")

''' LCEL 
combine all these into one chain. This chain will take input variables, pass those to a prompt template to create a prompt, pass the prompt to a language model, and then pass the output through an (optional) output parser. 
This is a convenient way to bundle up a modular piece of logic.
'''

template = """You are a helpful assistant who generates comma separated lists.
A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.
ONLY return a comma separated list, and nothing more."""
human_template = "{text}"

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", template),
    ("human", human_template),
])
chain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser()
# chain.invoke({"text": "colors"})

# 2. App definition
app = FastAPI(
  title="LangChain Server",
  version="1.0",
  description="A simple API server using LangChain's Runnable interfaces",
)

# 3. Adding chain route
add_routes(
    app,
    chain,
    path="/chain",
)

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="localhost", port=8000)